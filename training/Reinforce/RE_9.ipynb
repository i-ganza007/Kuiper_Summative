{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 31193,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "notebooka327ed1c20",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/i-ganza007/Kuiper_Summative/blob/main/training/Reinforce/RE_9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium pygame seaborn\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "Pd1mR2r6Nl1c"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Categorical\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import deque\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import json\n",
        "import time\n",
        "import pygame\n",
        "from typing import List, Dict, Tuple\n",
        "\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "print(\" All imports completed!\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-25T17:14:25.745007Z",
          "iopub.execute_input": "2025-11-25T17:14:25.745561Z",
          "iopub.status.idle": "2025-11-25T17:14:28.507868Z",
          "shell.execute_reply.started": "2025-11-25T17:14:25.745532Z",
          "shell.execute_reply": "2025-11-25T17:14:28.507017Z"
        },
        "id": "Vcrta8FdNl1e",
        "outputId": "871cf246-e7b9-4e44-a819-2bd71cb967dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": " All imports completed!\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pygame/pkgdata.py:25: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n  from pkg_resources import resource_stream, resource_exists\n/usr/local/lib/python3.11/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\nImplementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n  declare_namespace(pkg)\n/usr/local/lib/python3.11/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google.cloud')`.\nImplementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n  declare_namespace(pkg)\n/usr/local/lib/python3.11/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.\nImplementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n  declare_namespace(pkg)\n/usr/local/lib/python3.11/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('sphinxcontrib')`.\nImplementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n  declare_namespace(pkg)\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "OUTPUT_DIR = \"/kaggle/working\"\n",
        "LOG_DIR = f\"{OUTPUT_DIR}/logs/reinforce/\"\n",
        "CHECKPOINT_DIR = f\"{OUTPUT_DIR}/checkpoints_reinforce/\"\n",
        "RESULTS_DIR = f\"{OUTPUT_DIR}/results_reinforce/\"\n",
        "\n",
        "for d in [LOG_DIR, CHECKPOINT_DIR, RESULTS_DIR]:\n",
        "    os.makedirs(d, exist_ok=True)\n",
        "\n",
        "print(\"REINFORCE TRAINING - MISSION ENVIRONMENT\")\n",
        "print(f\"Output Directory: {OUTPUT_DIR}\")\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Device: {DEVICE}\")\n",
        "if DEVICE == \"cuda\":\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-25T17:14:37.89589Z",
          "iopub.execute_input": "2025-11-25T17:14:37.896338Z",
          "iopub.status.idle": "2025-11-25T17:14:37.980528Z",
          "shell.execute_reply.started": "2025-11-25T17:14:37.896314Z",
          "shell.execute_reply": "2025-11-25T17:14:37.979774Z"
        },
        "id": "sHBCqvrvNl1g",
        "outputId": "6e556cd0-5285-4304-8b7b-45a043a39e38"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "REINFORCE TRAINING - MISSION ENVIRONMENT\nOutput Directory: /kaggle/working\nDevice: cuda\nGPU: Tesla P100-PCIE-16GB\nMemory: 15.89 GB\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class MissionEnvironment(gym.Env):\n",
        "    \"\"\"Custom Mission Environment with grid-based tasks\"\"\"\n",
        "\n",
        "    metadata = {\"render_modes\": [\"human\", \"rgb_array\"], \"render_fps\": 30}\n",
        "\n",
        "    def __init__(self, grid_size=10, num_missions=3, render_mode=None):\n",
        "        super().__init__()\n",
        "        self.grid_size = grid_size\n",
        "        self.num_missions = num_missions\n",
        "        self.render_mode = render_mode\n",
        "\n",
        "        self.action_space = spaces.Discrete(5)\n",
        "\n",
        "        obs_size = 4 + (num_missions * 3)\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=np.array([0, 0, 0, 0] + [0, -grid_size, -grid_size] * num_missions, dtype=np.float32),\n",
        "            high=np.array([grid_size, grid_size, 200, num_missions] + [np.sqrt(2*grid_size**2), grid_size, grid_size] * num_missions, dtype=np.float32),\n",
        "            dtype=np.float32\n",
        "        )\n",
        "\n",
        "        self.agent_pos = None\n",
        "        self.missions = None\n",
        "        self.completed_missions = None\n",
        "        self.fuel = 200\n",
        "        self.max_fuel = 200\n",
        "        self.steps = 0\n",
        "        self.max_steps = 500\n",
        "        self.window = None\n",
        "        self.clock = None\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        super().reset(seed=seed)\n",
        "        self.agent_pos = np.array(\n",
        "            [self.np_random.integers(0, self.grid_size),\n",
        "             self.np_random.integers(0, self.grid_size)],\n",
        "            dtype=np.int32\n",
        "        )\n",
        "        self.missions = []\n",
        "        for _ in range(self.num_missions):\n",
        "            while True:\n",
        "                mission = np.array(\n",
        "                    [self.np_random.integers(0, self.grid_size),\n",
        "                     self.np_random.integers(0, self.grid_size)],\n",
        "                    dtype=np.int32\n",
        "                )\n",
        "                if not np.array_equal(mission, self.agent_pos) and \\\n",
        "                   not any(np.array_equal(mission, m) for m in self.missions):\n",
        "                    self.missions.append(mission)\n",
        "                    break\n",
        "        self.completed_missions = np.zeros(self.num_missions, dtype=bool)\n",
        "        self.fuel = self.max_fuel\n",
        "        self.steps = 0\n",
        "        return self._get_observation(), {}\n",
        "\n",
        "    def _get_observation(self):\n",
        "        obs = np.zeros(4 + self.num_missions * 3, dtype=np.float32)\n",
        "        obs[0] = float(self.agent_pos[0])\n",
        "        obs[1] = float(self.agent_pos[1])\n",
        "        obs[2] = float(self.fuel)\n",
        "        obs[3] = float(np.sum(self.completed_missions))\n",
        "\n",
        "        mission_idx = 0\n",
        "        for i in range(self.num_missions):\n",
        "            if not self.completed_missions[i]:\n",
        "                dx = float(self.missions[i][0] - self.agent_pos[0])\n",
        "                dy = float(self.missions[i][1] - self.agent_pos[1])\n",
        "                dist = np.sqrt(dx**2 + dy**2)\n",
        "                obs[4 + mission_idx*3] = dist\n",
        "                obs[4 + mission_idx*3 + 1] = dx\n",
        "                obs[4 + mission_idx*3 + 2] = dy\n",
        "            else:\n",
        "                obs[4 + mission_idx*3] = 0.0\n",
        "                obs[4 + mission_idx*3 + 1] = 0.0\n",
        "                obs[4 + mission_idx*3 + 2] = 0.0\n",
        "            mission_idx += 1\n",
        "\n",
        "        return obs\n",
        "\n",
        "    def step(self, action):\n",
        "        self.steps += 1\n",
        "        reward = -0.1\n",
        "        terminated = False\n",
        "        truncated = self.steps >= self.max_steps\n",
        "\n",
        "        if action < 4:\n",
        "            if self.fuel > 0:\n",
        "                deltas = [(-1, 0), (1, 0), (0, -1), (0, 1)][action]\n",
        "                new_pos = self.agent_pos + np.array(deltas, dtype=np.int32)\n",
        "\n",
        "                if 0 <= new_pos[0] < self.grid_size and 0 <= new_pos[1] < self.grid_size:\n",
        "                    self.agent_pos = new_pos\n",
        "                    self.fuel -= 1\n",
        "\n",
        "                    active_mission_indices = [i for i in range(self.num_missions)\n",
        "                                            if not self.completed_missions[i]]\n",
        "                    if active_mission_indices:\n",
        "                        distances = [np.linalg.norm(self.agent_pos.astype(float) - self.missions[i].astype(float))\n",
        "                                   for i in active_mission_indices]\n",
        "                        min_dist = np.min(distances)\n",
        "                        reward += 2.0 / (1.0 + min_dist)\n",
        "                else:\n",
        "                    reward -= 0.2\n",
        "            else:\n",
        "                reward -= 0.1\n",
        "\n",
        "        elif action == 4:\n",
        "            mission_completed = False\n",
        "            for i, mission in enumerate(self.missions):\n",
        "                if not self.completed_missions[i] and np.array_equal(self.agent_pos, mission):\n",
        "                    self.completed_missions[i] = True\n",
        "                    reward += 200.0\n",
        "                    mission_completed = True\n",
        "                    self.fuel = min(self.max_fuel, self.fuel + 30)\n",
        "                    break\n",
        "\n",
        "            if not mission_completed:\n",
        "                reward -= 0.5\n",
        "\n",
        "        completed_count = np.sum(self.completed_missions)\n",
        "        if completed_count == self.num_missions:\n",
        "            reward += 500.0\n",
        "            reward += self.fuel * 0.5\n",
        "            terminated = True\n",
        "\n",
        "        if self.fuel <= 0 and not terminated:\n",
        "            reward -= 10.0\n",
        "            terminated = True\n",
        "\n",
        "        if truncated and not terminated:\n",
        "            reward -= 5.0\n",
        "\n",
        "        return self._get_observation(), reward, terminated, truncated, {}\n",
        "\n",
        "    def render(self):\n",
        "        if self.render_mode is None:\n",
        "            return\n",
        "        if self.window is None and self.render_mode == \"human\":\n",
        "            import pygame\n",
        "            pygame.init()\n",
        "            self.window = pygame.display.set_mode((600, 600))\n",
        "            self.clock = pygame.time.Clock()\n",
        "\n",
        "        import pygame\n",
        "        canvas = pygame.Surface((600, 600))\n",
        "        canvas.fill((255, 255, 255))\n",
        "        pix = 600 / self.grid_size\n",
        "\n",
        "        for i, m in enumerate(self.missions):\n",
        "            color = (100, 200, 100) if self.completed_missions[i] else (200, 100, 100)\n",
        "            pygame.draw.rect(canvas, color, (m[0]*pix, m[1]*pix, pix, pix))\n",
        "\n",
        "        pygame.draw.circle(canvas, (0, 0, 255),\n",
        "                          ((self.agent_pos[0]+0.5)*pix, (self.agent_pos[1]+0.5)*pix),\n",
        "                          pix/3)\n",
        "\n",
        "        for x in range(self.grid_size + 1):\n",
        "            pygame.draw.line(canvas, (200, 200, 200), (x*pix, 0), (x*pix, 600))\n",
        "            pygame.draw.line(canvas, (200, 200, 200), (0, x*pix), (600, x*pix))\n",
        "\n",
        "        if self.render_mode == \"human\":\n",
        "            self.window.blit(canvas, (0, 0))\n",
        "            pygame.display.flip()\n",
        "            self.clock.tick(30)\n",
        "\n",
        "    def close(self):\n",
        "        if self.window:\n",
        "            import pygame\n",
        "            pygame.quit()\n",
        "            self.window = None\n",
        "\n",
        "gym.register(id=\"MissionEnv-v0\", entry_point=\"__main__:MissionEnvironment\", max_episode_steps=500)\n",
        "print(\" MissionEnvironment registered!\\n\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-25T17:14:42.316082Z",
          "iopub.execute_input": "2025-11-25T17:14:42.316412Z",
          "iopub.status.idle": "2025-11-25T17:14:42.3385Z",
          "shell.execute_reply.started": "2025-11-25T17:14:42.316389Z",
          "shell.execute_reply": "2025-11-25T17:14:42.337685Z"
        },
        "id": "Vz-Bji2pNl1h",
        "outputId": "b13864e8-76c4-4851-bf87-42295ca50407"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": " MissionEnvironment registered!\n\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class PolicyNetwork(nn.Module):\n",
        "    \"\"\"Neural network for REINFORCE policy\"\"\"\n",
        "\n",
        "    def __init__(self, state_dim, action_dim, hidden_dims=[256, 128]):\n",
        "        super(PolicyNetwork, self).__init__()\n",
        "\n",
        "        layers = []\n",
        "        input_dim = state_dim\n",
        "\n",
        "        for hidden_dim in hidden_dims:\n",
        "            layers.append(nn.Linear(input_dim, hidden_dim))\n",
        "            layers.append(nn.ReLU())\n",
        "            layers.append(nn.Dropout(0.1))\n",
        "            input_dim = hidden_dim\n",
        "\n",
        "        self.network = nn.Sequential(*layers)\n",
        "        self.action_head = nn.Linear(input_dim, action_dim)\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            nn.init.orthogonal_(module.weight, gain=0.01)\n",
        "            nn.init.constant_(module.bias, 0.0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.network(x)\n",
        "        action_scores = self.action_head(x)\n",
        "        return F.softmax(action_scores, dim=-1)\n",
        "\n",
        "print(\" Policy Network defined!\\n\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-25T17:14:53.183682Z",
          "iopub.execute_input": "2025-11-25T17:14:53.184439Z",
          "iopub.status.idle": "2025-11-25T17:14:53.190759Z",
          "shell.execute_reply.started": "2025-11-25T17:14:53.184414Z",
          "shell.execute_reply": "2025-11-25T17:14:53.190038Z"
        },
        "id": "WQCzpR-MNl1j",
        "outputId": "698d5ad2-34ee-4a65-c5ae-7d73b18fcd74"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": " Policy Network defined!\n\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class REINFORCE:\n",
        "    \"\"\"\n",
        "    REINFORCE Algorithm (Monte Carlo Policy Gradient)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, state_dim, action_dim, config):\n",
        "        self.config = config\n",
        "        self.device = torch.device(DEVICE)\n",
        "\n",
        "        self.policy = PolicyNetwork(state_dim, action_dim, config['hidden_dims']).to(self.device)\n",
        "        self.optimizer = optim.Adam(self.policy.parameters(),\n",
        "                                  lr=config['learning_rate'],\n",
        "                                  weight_decay=config.get('weight_decay', 0.0))\n",
        "\n",
        "        self.saved_log_probs = []\n",
        "        self.rewards = []\n",
        "        self.entropies = []\n",
        "\n",
        "        self.episode_rewards = []\n",
        "        self.episode_lengths = []\n",
        "\n",
        "    def select_action(self, state):\n",
        "        \"\"\"Select action using current policy\"\"\"\n",
        "        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
        "        probs = self.policy(state)\n",
        "        m = Categorical(probs)\n",
        "        action = m.sample()\n",
        "\n",
        "        self.saved_log_probs.append(m.log_prob(action))\n",
        "        self.entropies.append(m.entropy())\n",
        "\n",
        "        return action.item()\n",
        "\n",
        "    def compute_returns(self, rewards, normalize=True):\n",
        "        \"\"\"Compute discounted returns\"\"\"\n",
        "        returns = []\n",
        "        G = 0\n",
        "\n",
        "        for r in reversed(rewards):\n",
        "            G = r + self.config['gamma'] * G\n",
        "            returns.insert(0, G)\n",
        "\n",
        "        returns = torch.tensor(returns, device=self.device)\n",
        "\n",
        "        if normalize:\n",
        "            returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
        "\n",
        "        return returns\n",
        "\n",
        "    def update_policy(self):\n",
        "        \"\"\"Update policy using REINFORCE gradient\"\"\"\n",
        "        returns = self.compute_returns(self.rewards, self.config.get('normalize_returns', True))\n",
        "\n",
        "        policy_loss = []\n",
        "        for log_prob, G in zip(self.saved_log_probs, returns):\n",
        "            policy_loss.append(-log_prob * G)\n",
        "\n",
        "        entropy_loss = -torch.stack(self.entropies).mean() * self.config.get('entropy_coef', 0.01)\n",
        "\n",
        "        loss = torch.stack(policy_loss).sum() + entropy_loss\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        if self.config.get('max_grad_norm'):\n",
        "            torch.nn.utils.clip_grad_norm_(self.policy.parameters(), self.config['max_grad_norm'])\n",
        "\n",
        "        self.optimizer.step()\n",
        "\n",
        "        self.saved_log_probs = []\n",
        "        self.rewards = []\n",
        "        self.entropies = []\n",
        "\n",
        "        return loss.item()\n",
        "\n",
        "    def save_checkpoint(self, path):\n",
        "        \"\"\"Save model checkpoint\"\"\"\n",
        "        torch.save({\n",
        "            'policy_state_dict': self.policy.state_dict(),\n",
        "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "            'config': self.config\n",
        "        }, path)\n",
        "\n",
        "    def load_checkpoint(self, path):\n",
        "        \"\"\"Load model checkpoint\"\"\"\n",
        "        checkpoint = torch.load(path, map_location=self.device)\n",
        "        self.policy.load_state_dict(checkpoint['policy_state_dict'])\n",
        "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "print(\" REINFORCE Algorithm defined!\\n\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-25T17:15:03.306304Z",
          "iopub.execute_input": "2025-11-25T17:15:03.306935Z",
          "iopub.status.idle": "2025-11-25T17:15:03.317397Z",
          "shell.execute_reply.started": "2025-11-25T17:15:03.306907Z",
          "shell.execute_reply": "2025-11-25T17:15:03.316525Z"
        },
        "id": "0AyVW3ctNl1l",
        "outputId": "0f366b98-939c-49c5-98e0-18e2c82edc19"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": " REINFORCE Algorithm defined!\n\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "CONFIG_REINFORCE = {\n",
        "        \"learning_rate\": 0.0002,\n",
        "        \"gamma\": 0.999,\n",
        "        \"hidden_dims\": [256, 256, 128],\n",
        "        \"entropy_coef\": 0.002,\n",
        "        \"normalize_returns\": True,\n",
        "        \"max_grad_norm\": 0.1,\n",
        "        \"weight_decay\": 0.002,\n",
        "        \"total_episodes\": 2000,\n",
        "        \"checkpoint_freq\": 500\n",
        "}\n",
        "\n",
        "print(\"REINFORCE Configuration:\")\n",
        "for key, val in CONFIG_REINFORCE.items():\n",
        "    print(f\"  {key}: {val}\")\n",
        "print()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-25T17:38:47.788328Z",
          "iopub.execute_input": "2025-11-25T17:38:47.788936Z",
          "iopub.status.idle": "2025-11-25T17:38:47.794066Z",
          "shell.execute_reply.started": "2025-11-25T17:38:47.78891Z",
          "shell.execute_reply": "2025-11-25T17:38:47.793363Z"
        },
        "id": "J1KmIK-WNl1u",
        "outputId": "d6a122ea-8851-4fc4-92cf-351b096d48b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "REINFORCE Configuration:\n  learning_rate: 0.0002\n  gamma: 0.999\n  hidden_dims: [256, 256, 128]\n  entropy_coef: 0.002\n  normalize_returns: True\n  max_grad_norm: 0.1\n  weight_decay: 0.002\n  total_episodes: 2000\n  checkpoint_freq: 500\n\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class TrainingMetrics:\n",
        "    \"\"\"Comprehensive training metrics tracking\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.episode_rewards = []\n",
        "        self.episode_lengths = []\n",
        "        self.episode_losses = []\n",
        "        self.missions_completed = []\n",
        "        self.successes = []\n",
        "        self.timesteps = []\n",
        "        self.start_time = time.time()\n",
        "\n",
        "    def update(self, episode_reward, episode_length, loss, completed_missions, timestep):\n",
        "        \"\"\"Update metrics after each episode\"\"\"\n",
        "        self.episode_rewards.append(episode_reward)\n",
        "        self.episode_lengths.append(episode_length)\n",
        "        self.episode_losses.append(loss)\n",
        "        self.missions_completed.append(completed_missions)\n",
        "        self.successes.append(1 if completed_missions == 3 else 0)\n",
        "        self.timesteps.append(timestep)\n",
        "\n",
        "    def get_summary_stats(self, window=100):\n",
        "        \"\"\"Get summary statistics\"\"\"\n",
        "        if len(self.episode_rewards) == 0:\n",
        "            return {}\n",
        "\n",
        "        recent_rewards = self.episode_rewards[-window:]\n",
        "        recent_lengths = self.episode_lengths[-window:]\n",
        "        recent_successes = self.successes[-window:]\n",
        "\n",
        "        return {\n",
        "            'mean_reward': np.mean(recent_rewards),\n",
        "            'std_reward': np.std(recent_rewards),\n",
        "            'mean_steps': np.mean(recent_lengths),\n",
        "            'success_rate': np.mean(recent_successes) * 100,\n",
        "            'total_episodes': len(self.episode_rewards),\n",
        "            'training_time': time.time() - self.start_time\n",
        "        }\n",
        "\n",
        "    def save_to_file(self, filepath):\n",
        "        \"\"\"Save metrics to JSON file\"\"\"\n",
        "        data = {\n",
        "            'episode_rewards': self.episode_rewards,\n",
        "            'episode_lengths': self.episode_lengths,\n",
        "            'episode_losses': self.episode_losses,\n",
        "            'missions_completed': self.missions_completed,\n",
        "            'successes': self.successes,\n",
        "            'timesteps': self.timesteps,\n",
        "            'summary_stats': self.get_summary_stats()\n",
        "        }\n",
        "\n",
        "        with open(filepath, 'w') as f:\n",
        "            json.dump(data, f, indent=2)\n",
        "\n",
        "print(\" Training Metrics defined!\\n\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-25T17:38:51.63871Z",
          "iopub.execute_input": "2025-11-25T17:38:51.639521Z",
          "iopub.status.idle": "2025-11-25T17:38:51.648035Z",
          "shell.execute_reply.started": "2025-11-25T17:38:51.639492Z",
          "shell.execute_reply": "2025-11-25T17:38:51.64714Z"
        },
        "id": "fZLc_QF-Nl1v",
        "outputId": "ae36c5aa-dc2a-424c-fed1-3d4fd931be77"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": " Training Metrics defined!\n\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def train_reinforce(config, env):\n",
        "    \"\"\"Main training function for REINFORCE\"\"\"\n",
        "\n",
        "    print(\"STARTING REINFORCE TRAINING\")\n",
        "    print(f\"Total Episodes: {config['total_episodes']:,}\\n\")\n",
        "\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.n\n",
        "    agent = REINFORCE(state_dim, action_dim, config)\n",
        "    metrics = TrainingMetrics()\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    for episode in range(1, config['total_episodes'] + 1):\n",
        "        state, _ = env.reset()\n",
        "        episode_reward = 0\n",
        "        episode_length = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action = agent.select_action(state)\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            agent.rewards.append(reward)\n",
        "            episode_reward += reward\n",
        "            episode_length += 1\n",
        "            state = next_state\n",
        "\n",
        "        loss = agent.update_policy()\n",
        "\n",
        "        completed_missions = int(state[3])\n",
        "\n",
        "        metrics.update(episode_reward, episode_length, loss, completed_missions, episode)\n",
        "\n",
        "        if episode % config['checkpoint_freq'] == 0:\n",
        "            checkpoint_path = f\"{CHECKPOINT_DIR}reinforce_episode_{episode}.pth\"\n",
        "            agent.save_checkpoint(checkpoint_path)\n",
        "\n",
        "        if episode % 100 == 0 or episode == config['total_episodes']:\n",
        "            stats = metrics.get_summary_stats(window=100)\n",
        "            print(f\"Episode {episode:4d}/{config['total_episodes']} | \"\n",
        "                  f\"Reward: {episode_reward:7.2f} | \"\n",
        "                  f\"Avg Reward: {stats['mean_reward']:6.2f} | \"\n",
        "                  f\"Success: {stats['success_rate']:5.1f}% | \"\n",
        "                  f\"Loss: {loss:.4f}\")\n",
        "\n",
        "    training_time = time.time() - start_time\n",
        "\n",
        "    print(\"REINFORCE TRAINING COMPLETED!\")\n",
        "    print(f\"Training time: {training_time:.2f} seconds ({training_time/60:.2f} minutes)\")\n",
        "\n",
        "    return agent, metrics\n",
        "\n",
        "env = MissionEnvironment()\n",
        "print(f\" Environment created: {env.observation_space.shape} -> {env.action_space.n} actions\\n\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-25T17:38:59.593438Z",
          "iopub.execute_input": "2025-11-25T17:38:59.593975Z",
          "iopub.status.idle": "2025-11-25T17:38:59.602505Z",
          "shell.execute_reply.started": "2025-11-25T17:38:59.593949Z",
          "shell.execute_reply": "2025-11-25T17:38:59.601762Z"
        },
        "id": "N9AzTkjoNl1w",
        "outputId": "a238716d-f9ca-42b3-ea2f-13bd788dc987"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": " Environment created: (13,) -> 5 actions\n\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "agent, metrics = train_reinforce(CONFIG_REINFORCE, env)\n",
        "\n",
        "final_model_path = f\"{OUTPUT_DIR}/reinforce_eight.pth\"\n",
        "agent.save_checkpoint(final_model_path)\n",
        "print(f\" Final model saved: {final_model_path}\")\n",
        "\n",
        "metrics_path = f\"{RESULTS_DIR}/reinforce_metrics.json\"\n",
        "metrics.save_to_file(metrics_path)\n",
        "print(f\" Metrics saved: {metrics_path}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-25T17:39:21.895472Z",
          "iopub.execute_input": "2025-11-25T17:39:21.896274Z",
          "iopub.status.idle": "2025-11-25T18:02:47.283897Z",
          "shell.execute_reply.started": "2025-11-25T17:39:21.896247Z",
          "shell.execute_reply": "2025-11-25T18:02:47.282981Z"
        },
        "id": "4_aQM8KKNl1x",
        "outputId": "6526b76a-d601-4193-c7f4-c28630361f84"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "STARTING REINFORCE TRAINING\nTotal Episodes: 2,000\n\nEpisode  100/2000 | Reward:  250.26 | Avg Reward: 264.09 | Success:   7.0% | Loss: -0.0021\nEpisode  200/2000 | Reward:  257.05 | Avg Reward: 267.44 | Success:   7.0% | Loss: 0.0292\nEpisode  300/2000 | Reward:  232.97 | Avg Reward: 247.41 | Success:   6.0% | Loss: -0.0109\nEpisode  400/2000 | Reward:  264.11 | Avg Reward: 244.85 | Success:   5.0% | Loss: 0.0224\nEpisode  500/2000 | Reward:  280.61 | Avg Reward: 248.98 | Success:   5.0% | Loss: -0.0654\nEpisode  600/2000 | Reward:   17.89 | Avg Reward: 260.48 | Success:   6.0% | Loss: -0.0077\nEpisode  700/2000 | Reward:  440.40 | Avg Reward: 255.18 | Success:   5.0% | Loss: -0.0597\nEpisode  800/2000 | Reward:    1.93 | Avg Reward: 247.47 | Success:   3.0% | Loss: -0.0496\nEpisode  900/2000 | Reward:  476.37 | Avg Reward: 268.40 | Success:   7.0% | Loss: -0.0883\nEpisode 1000/2000 | Reward:   77.03 | Avg Reward: 258.27 | Success:   5.0% | Loss: 0.1495\nEpisode 1100/2000 | Reward:  270.04 | Avg Reward: 272.21 | Success:   6.0% | Loss: -0.1044\nEpisode 1200/2000 | Reward:   54.31 | Avg Reward: 226.04 | Success:   3.0% | Loss: 0.0260\nEpisode 1300/2000 | Reward:  223.22 | Avg Reward: 214.49 | Success:   3.0% | Loss: -0.1302\nEpisode 1400/2000 | Reward:   31.19 | Avg Reward: 244.09 | Success:   4.0% | Loss: 0.1367\nEpisode 1500/2000 | Reward:  432.49 | Avg Reward: 288.47 | Success:   5.0% | Loss: -0.0406\nEpisode 1600/2000 | Reward:  291.77 | Avg Reward: 217.48 | Success:   2.0% | Loss: 0.0755\nEpisode 1700/2000 | Reward:  470.28 | Avg Reward: 240.23 | Success:   3.0% | Loss: 0.1200\nEpisode 1800/2000 | Reward:   80.70 | Avg Reward: 293.41 | Success:   6.0% | Loss: -0.0635\nEpisode 1900/2000 | Reward:  452.35 | Avg Reward: 236.34 | Success:   4.0% | Loss: -0.2123\nEpisode 2000/2000 | Reward:  -12.35 | Avg Reward: 251.47 | Success:   5.0% | Loss: 0.2583\nREINFORCE TRAINING COMPLETED!\nTraining time: 1404.80 seconds (23.41 minutes)\n Final model saved: /kaggle/working/reinforce_eight.pth\n Metrics saved: /kaggle/working/results_reinforce//reinforce_metrics.json\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_reinforce(agent, num_episodes=50):\n",
        "    \"\"\"Evaluate trained REINFORCE agent\"\"\"\n",
        "\n",
        "    print(f\"\\nEvaluating REINFORCE on {num_episodes} episodes...\")\n",
        "\n",
        "    eval_env = MissionEnvironment()\n",
        "    total_rewards = []\n",
        "    success_count = 0\n",
        "    episode_lengths = []\n",
        "\n",
        "    for ep in range(num_episodes):\n",
        "        state, _ = eval_env.reset()\n",
        "        episode_reward = 0\n",
        "        episode_length = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action = agent.select_action(state)\n",
        "            state, reward, terminated, truncated, _ = eval_env.step(action)\n",
        "            episode_reward += reward\n",
        "            episode_length += 1\n",
        "            done = terminated or truncated\n",
        "\n",
        "        total_rewards.append(episode_reward)\n",
        "        episode_lengths.append(episode_length)\n",
        "\n",
        "        completed_missions = int(state[3])\n",
        "        if completed_missions == 3:\n",
        "            success_count += 1\n",
        "\n",
        "        if (ep + 1) % 10 == 0:\n",
        "            print(f\"  Episode {ep+1:2d}: Reward={episode_reward:7.2f}, \"\n",
        "                  f\"Missions={completed_missions}/3\")\n",
        "\n",
        "    eval_env.close()\n",
        "\n",
        "    mean_reward = np.mean(total_rewards)\n",
        "    std_reward = np.std(total_rewards)\n",
        "    success_rate = (success_count / num_episodes) * 100\n",
        "    avg_length = np.mean(episode_lengths)\n",
        "\n",
        "    print(f\"\\n Evaluation Results:\")\n",
        "    print(f\"  Mean Reward: {mean_reward:.2f} ± {std_reward:.2f}\")\n",
        "    print(f\"  Success Rate: {success_rate:.1f}%\")\n",
        "    print(f\"  Avg Episode Length: {avg_length:.1f} steps\")\n",
        "\n",
        "    return {\n",
        "        'mean_reward': mean_reward,\n",
        "        'std_reward': std_reward,\n",
        "        'success_rate': success_rate,\n",
        "        'avg_length': avg_length,\n",
        "        'all_rewards': total_rewards\n",
        "    }\n",
        "\n",
        "eval_results = evaluate_reinforce(agent, num_episodes=50)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-25T18:03:46.914564Z",
          "iopub.execute_input": "2025-11-25T18:03:46.914877Z",
          "iopub.status.idle": "2025-11-25T18:04:08.18395Z",
          "shell.execute_reply.started": "2025-11-25T18:03:46.914853Z",
          "shell.execute_reply": "2025-11-25T18:04:08.183113Z"
        },
        "id": "yT4uZWb3Nl1x",
        "outputId": "68e5ec40-9073-46d7-a233-0caee1821635"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "\nEvaluating REINFORCE on 50 episodes...\n  Episode 10: Reward= 435.73, Missions=2/3\n  Episode 20: Reward= 435.08, Missions=2/3\n  Episode 30: Reward= 232.37, Missions=1/3\n  Episode 40: Reward= 246.74, Missions=1/3\n  Episode 50: Reward=  48.88, Missions=0/3\n\n Evaluation Results:\n  Mean Reward: 275.91 ± 301.47\n  Success Rate: 8.0%\n  Avg Episode Length: 307.6 steps\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def create_training_visualization(metrics, eval_results, config):\n",
        "    \"\"\"Create comprehensive training visualization\"\"\"\n",
        "\n",
        "    fig = plt.figure(figsize=(20, 12))\n",
        "    fig.suptitle('REINFORCE Training Results - Mission Environment',\n",
        "                 fontsize=16, fontweight='bold', y=0.98)\n",
        "\n",
        "    episodes = range(1, len(metrics.episode_rewards) + 1)\n",
        "    rewards = np.array(metrics.episode_rewards)\n",
        "    lengths = np.array(metrics.episode_lengths)\n",
        "    successes = np.array(metrics.successes)\n",
        "\n",
        "    ax1 = plt.subplot(2, 3, 1)\n",
        "    ax1.plot(episodes, rewards, alpha=0.5, label='Episode Reward', linewidth=1, color='blue')\n",
        "    if len(rewards) > 50:\n",
        "        rolling = np.convolve(rewards, np.ones(50)/50, mode='valid')\n",
        "        ax1.plot(episodes[49:], rolling, linewidth=2.5, label='50-ep Avg', color='darkblue')\n",
        "    ax1.axhline(y=500, color='g', linestyle='--', linewidth=2, alpha=0.7, label='Success Threshold')\n",
        "    ax1.set_xlabel('Episode', fontsize=11, fontweight='bold')\n",
        "    ax1.set_ylabel('Reward', fontsize=11, fontweight='bold')\n",
        "    ax1.set_title('Episode Rewards Over Training', fontsize=12, fontweight='bold')\n",
        "    ax1.legend(loc='best')\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "    ax2 = plt.subplot(2, 3, 2)\n",
        "    ax2.plot(episodes, lengths, alpha=0.5, color='orange', linewidth=1)\n",
        "    if len(lengths) > 50:\n",
        "        rolling_len = np.convolve(lengths, np.ones(50)/50, mode='valid')\n",
        "        ax2.plot(episodes[49:], rolling_len, linewidth=2.5, color='darkorange', label='50-ep Avg')\n",
        "    ax2.set_xlabel('Episode', fontsize=11, fontweight='bold')\n",
        "    ax2.set_ylabel('Steps per Episode', fontsize=11, fontweight='bold')\n",
        "    ax2.set_title('Episode Length Over Training', fontsize=12, fontweight='bold')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "\n",
        "    ax3 = plt.subplot(2, 3, 3)\n",
        "    cum_success = np.cumsum(successes)\n",
        "    success_pct = (cum_success / np.arange(1, len(cum_success)+1)) * 100\n",
        "    ax3.plot(episodes, success_pct, linewidth=2.5, color='green')\n",
        "    ax3.fill_between(episodes, 0, success_pct, alpha=0.3, color='green')\n",
        "    ax3.set_xlabel('Episode', fontsize=11, fontweight='bold')\n",
        "    ax3.set_ylabel('Success Rate (%)', fontsize=11, fontweight='bold')\n",
        "    ax3.set_title('Cumulative Success Rate', fontsize=12, fontweight='bold')\n",
        "    ax3.set_ylim([0, 100])\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "\n",
        "    ax4 = plt.subplot(2, 3, 4)\n",
        "    final_rewards = rewards[-min(100, len(rewards)):]\n",
        "    ax4.hist(final_rewards, bins=20, color='lightblue', edgecolor='black', alpha=0.7)\n",
        "    ax4.axvline(np.mean(final_rewards), color='red', linestyle='--', linewidth=2.5,\n",
        "               label=f'Mean: {np.mean(final_rewards):.1f}')\n",
        "    ax4.set_xlabel('Reward', fontsize=11, fontweight='bold')\n",
        "    ax4.set_ylabel('Frequency', fontsize=11, fontweight='bold')\n",
        "    ax4.set_title('Final Episodes Reward Distribution', fontsize=12, fontweight='bold')\n",
        "    ax4.legend()\n",
        "    ax4.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "    ax5 = plt.subplot(2, 3, 5)\n",
        "    ax5.axis('off')\n",
        "\n",
        "    stats = metrics.get_summary_stats()\n",
        "    final_success_rate = np.mean(successes[-100:]) * 100 if len(successes) > 100 else np.mean(successes) * 100\n",
        "\n",
        "    summary_text = f\"\"\"\n",
        "REINFORCE TRAINING SUMMARY\n",
        "{'─'*40}\n",
        "Total Episodes: {config['total_episodes']:,}\n",
        "Training Time: {stats['training_time']:.1f}s\n",
        "\n",
        "FINAL PERFORMANCE:\n",
        "Mean Reward: {stats['mean_reward']:.2f}\n",
        "Std Reward: {stats['std_reward']:.2f}\n",
        "Success Rate: {final_success_rate:.1f}%\n",
        "Avg Episode Length: {stats['mean_steps']:.1f}\n",
        "\n",
        "EVALUATION RESULTS:\n",
        "Mean Reward: {eval_results['mean_reward']:.2f}\n",
        "Success Rate: {eval_results['success_rate']:.1f}%\n",
        "\n",
        "HYPERPARAMETERS:\n",
        "Learning Rate: {config['learning_rate']}\n",
        "Gamma: {config['gamma']}\n",
        "Hidden Layers: {config['hidden_dims']}\n",
        "Entropy Coef: {config['entropy_coef']}\n",
        "Device: {DEVICE}\n",
        "\"\"\"\n",
        "\n",
        "    ax5.text(0.05, 0.95, summary_text, fontsize=10, family='monospace',\n",
        "            verticalalignment='top', bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))\n",
        "\n",
        "    ax6 = plt.subplot(2, 3, 6)\n",
        "    mission_counts = [metrics.missions_completed.count(i) for i in range(4)]\n",
        "    colors = ['red', 'orange', 'yellow', 'green']\n",
        "    bars = ax6.bar(range(4), mission_counts, color=colors, alpha=0.7, edgecolor='black')\n",
        "\n",
        "    for i, bar in enumerate(bars):\n",
        "        height = bar.get_height()\n",
        "        ax6.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                f'{int(height)}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "    ax6.set_xlabel('Missions Completed', fontsize=11, fontweight='bold')\n",
        "    ax6.set_ylabel('Number of Episodes', fontsize=11, fontweight='bold')\n",
        "    ax6.set_title('Mission Completion Distribution', fontsize=12, fontweight='bold')\n",
        "    ax6.set_xticks(range(4))\n",
        "    ax6.set_xticklabels(['0', '1', '2', '3'])\n",
        "    ax6.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    output_path = f\"{OUTPUT_DIR}/reinforce_training_results.png\"\n",
        "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
        "    print(f\"Saved visualization: {output_path}\")\n",
        "    plt.show()\n",
        "\n",
        "create_training_visualization(metrics, eval_results, CONFIG_REINFORCE)"
      ],
      "metadata": {
        "trusted": true,
        "id": "aZegnGZGNl10"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}